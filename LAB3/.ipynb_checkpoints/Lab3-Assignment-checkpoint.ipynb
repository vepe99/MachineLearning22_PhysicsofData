{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 3\n",
    "# IMAGE CLASSIFICATION WITH NEURAL NETWORKS\n",
    "Course 2022/23: P. Zanuttigh, F. Barbato, M. Mel\n",
    "\n",
    "The notebook contains some simple tasks to be performed with **NEURAL NETWORKS (NNs)**. <br>\n",
    "Complete all the **required code sections** and **answer to all the questions**. <br>\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "1. **Read carefully all cells** and **follow the instructions**\n",
    "2. **Re-run all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "3. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "4. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command**.\n",
    "5. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes Classification with Neural Networks\n",
    "\n",
    "In this notebook we are going to explore the Neural Networks for image classification. We are going to use [**Fashion MNIST**](https://github.com/zalandoresearch/fashion-mnist), a dataset of small images of clothes and accessories.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required packages and check Scikit-learn version\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "print ('scikit-learn version: ', sklearn.__version__)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#from sklearnex import patch_sklearn # if you have an Intel CPU and have installed the Intel MKL \n",
    "#patch_sklearn()                     # library you can uncomment these lines for a speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load Fashion MNIST dataset from disk\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB3.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Mario Rossi<br>\n",
    "**ID Number**: 1234567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.0:** **Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: `ID = 1234567 + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1234567\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset**. <br>\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above.<br><br>\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "X, y = load_mnist(\"data\")\n",
    "print(\"Number of samples in the MNIST dataset:\", X.shape[0])\n",
    "# rescale the data to [0, 1]\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.1:** **Divide** the **data into training and test set** using a **500** samples in the **training set**. <br>\n",
    "Make sure that **each label** is present at **least 10 times** in training frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly permute the data and split into training and test taking \n",
    "# the first 500 data samples as training and the rests as test\n",
    "permutation = # ADD YOUR CODE HERE\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = # ADD YOUR CODE HERE\n",
    "\n",
    "X_train, X_test = # ADD YOUR CODE HERE\n",
    "y_train, y_test = # ADD YOUR CODE HERE\n",
    "\n",
    "labels, freqs = # ADD YOUR CODE HERE\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the plotting function\n",
    "plot_input(X_train,y_train,10)\n",
    "plot_input(X_test,y_test,100)\n",
    "plot_input(X_test,y_test,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.2** Now **use a feed-forward Neural Network** for prediction. <br><br>\n",
    "Use the **multi-layer perceptron** classifier, with the following parameters: <br>\n",
    "`max_iter=250, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, random_state=ID` <br>\n",
    "The `random_state` parameter ensures the run is the same even if you run it more than once, while the `alpha` parameter is the regularization term.<br>\n",
    "You might get some **warnings about the convergence**, ignore them (or try to increase the max_iter parameter if you have a powerful computer). <br>\n",
    "\n",
    "Then, using the default activation function, **pick four or five architectures** to consider, with different numbers of hidden layers and different sizes. <br>\n",
    "It is not necessary to create huge neural networks, you can limit to 3 layers and, for each layer, its maximum size can be of 100. <br>\n",
    "Evaluate the architectures you chose using **GridSearchCV with cv=5**. <br>\n",
    "\n",
    "You can reduce the number of iterations if the running time is too long on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are sample values but feel free to change them as you like, try to experiment with different sizes!!\n",
    "parameters = {'hidden_layer_sizes': [(10,), (25,), (50,), (25,10,), (50,25,10) ]}\n",
    "\n",
    "mlp = # ADD YOUR CODE HERE\n",
    "\n",
    "mlp_arch_CV = # ADD YOUR CODE HERE\n",
    "mlp_arch_CV.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"\\nMean scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.3** Now **try different batch sizes**, while keeping the **best NN** architecture you have found above. <br>\n",
    "Remember that the batch size was previously set to the default value, i.e., `min(200, n_samples)`. <br>\n",
    "\n",
    "Recall that a **batch size of 1 corresponds to baseline SGD**, while using all the **400 training samples** (there are 500 samples but in cross validation with 5 folders we use 1/5 of them for validation at each round) corresponds to **standard GD** and using a different mini-batch size lies in the middle between the two extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are sample values corresponding to baseline SGD, a reasonable mini-batch size and standard GD\n",
    "# again feel free to change them as you like, try to experiment with different batch sizes!!\n",
    "parameters = {'batch_size': [1, 32, 400]}\n",
    "\n",
    "# need to specify that you would like to use the standard k-fold split otherwise sklearn create splits of different sizes\n",
    "kf =  # ADD YOUR CODE HERE, suggestion: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "best_hidden_layer_sizes = # ADD YOUR CODE HERE\n",
    "mlp = # ADD YOUR CODE HERE\n",
    "\n",
    "# recall to use cv=kf to use the k-fold subdivision seen in the lectures\n",
    "mlp_batch_CV = # ADD YOUR CODE HERE\n",
    "mlp_batch_CV.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.Q1**\n",
    "\n",
    "- What do you observe for different architectures and batch sizes? <br>\n",
    "- How do the number of layers and their sizes affect the performances? <br>\n",
    "- What do you observe for different batch sizes, in particular what happens to the training convergence for different batch sizes (notice that the algorithm could not converge for some batch sizes)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q1**:<br>\n",
    "answer here\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.4** Now **try** to use **different learning rates**, while keeping the **best NN architecture and batch size you have found above.** <br>\n",
    "**Plot the learning curves** (i.e., the variation of the loss over the steps, you can get it from the loss_curve_ object of sklearn) for the different values of the learning rate. <br>\n",
    "You might get warnings about the convergence, this is expected. Ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [4*10**exp for exp in range(-4,0)]\n",
    "scores = {}\n",
    "\n",
    "best_hidden_layer_sizes = # ADD YOUR CODE HERE\n",
    "best_batch_size = # ADD YOUR CODE HERE\n",
    "\n",
    "for lr in lr_list: \n",
    "    mlp = # ADD YOUR CODE HERE\n",
    "    mlp.fit(X_train, y_train)\n",
    "    scores[lr] = # ADD YOUR CODE HERE, score on the test set\n",
    "    plt.plot(mlp.loss_curve_, label='lr: ' + str(lr))\n",
    "\n",
    "plt.legend(loc = 1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "print(\"Best parameters set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question A.Q2** Comment about the learning curves (i.e. the variation of the loss over the steps). <br>\n",
    "How does the curve changes for different learning rates in terms of stability and speed of convergence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER A.Q2**:<br>\n",
    "answer here\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO A.5** Now get **training and test error** for a NN with best parameters (architecture, batch size and learning rate)from above. Plot the learning curve also for this case. <br>\n",
    "As before, you might get a convergence warning, you can safely ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "mlp = # ADD YOUR CODE HERE\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "training_error = # ADD YOUR CODE HERE\n",
    "test_error = # ADD YOUR CODE HERE\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)\n",
    "\n",
    "plt.plot(mlp.loss_curve_, label=\"loss\")\n",
    "plt.legend(loc = 1)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) More data \n",
    "Now let's do the same but **using 20000** (or less if it takes too long on your machine) **data points for training**. <br>\n",
    "Make sure you are **consistent with the choice of `m_training`** in this and the later cells. <br>\n",
    "Use the **same NN architecture** as before, but you can try more if you like and have a powerful computer!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = # ADD YOUR CODE HERE\n",
    "\n",
    "X_train, X_test = # ADD YOUR CODE HERE\n",
    "y_train, y_test = # ADD YOUR CODE HERE\n",
    "\n",
    "labels, freqs = # ADD YOUR CODE HERE\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO B.1** Train the NNs with the added data points using the optimum parameters found above. <br>\n",
    "Eventually, feel free to try different architectures if you like. <br>\n",
    "We suggest that you use `verbose=True` so have an idea of how long it takes to run 1 iteration (eventually reduce also the number of iterations to 50). <br>\n",
    "This is just for debug purposes, **remember to switch it off** once you found a setup you like. <br>\n",
    "As before, you might get a convergence warning (especially if you reduced the number of iterations), you can safely ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_large = # ADD YOUR CODE HERE\n",
    "mlp_large.fit(X_train, y_train)\n",
    "\n",
    "print ('\\nRESULTS FOR NN\\n')\n",
    "\n",
    "#get training and test error for the NN\n",
    "training_error = # ADD YOUR CODE HERE\n",
    "test_error = # ADD YOUR CODE HERE\n",
    "print (\"NN training error: %f\" % training_error)\n",
    "print (\"NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question B.Q1** Compare the train and test error you got with a large number of samples with the best one you obtained with only 500 data points. Comment about the results you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER B.Q1**:<br>\n",
    "answer here\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO B.2** Plot an example that was missclassified by NN with m=500 training data points and it is now instead correctly classified by NN with m=20000 training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_prediction = # ADD YOUR CODE HERE\n",
    "large_NN_prediction = # ADD YOUR CODE HERE\n",
    "\n",
    "for i, (p, pl, l) in enumerate(zip(NN_prediction, large_NN_prediction, y_test)):\n",
    "    if p != l and pl == l:\n",
    "        plot_input(X_test, y_test, i)\n",
    "        print(\"NN prediction for m=500:\", p)\n",
    "        print(\"NN prediction for m=20000:\", pl)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO B.3** Plot the weigths of the multi-layer perceptron classifier, for the best NN we get with 500 data points and with 20000 data points. <br>\n",
    "Note that the code is provided, you just need to change the `mlp` variable name to the one used before - if you changed it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights with 500 data points:\")\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Weights with 20000 data points:\")\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "vmin, vmax = mlp_large.coefs_[0].min(), mlp_large.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question B.Q2** Describe what do you observe by looking at the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER B.Q2**:<br>\n",
    "answer here\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO B.4** Create a SVM classifier with the following parameters: `kernel='rbf', C=10, gamma=0.01`. <br>\n",
    "Fit it on a few data points and compute its training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_training = # ADD YOUR CODE HERE\n",
    "\n",
    "X_train, X_test = # ADD YOUR CODE HERE\n",
    "y_train, y_test = # ADD YOUR CODE HERE\n",
    "\n",
    "# best parameters found in the SVM notebook\n",
    "SVM = # ADD YOUR CODE HERE\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR SVM')\n",
    "SVM_training_error = # ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Training score SVM:\")\n",
    "print(SVM_training_error)\n",
    "\n",
    "SVM_test_error = # ADD YOUR CODE HERE\n",
    "print(\"Test score SVM:\")\n",
    "print(SVM_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question B.Q3** Compare the results of SVM and of NN. Which one achieves the best performances? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**ANSWER B.Q3**:<br>\n",
    "answer here\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
